# Optimizing LLM Training: An Overview of Techniques üëê üìö

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/m9v01CkHNjLKvt1eUHTWz.png)

Training large language models (LLMs) requires significant computational resources and time. However, by optimizing the training process, it's possible to cut costs, speed up development, and improve the model's overall performance. This guide offers a detailed exploration of various optimization strategies, covering everything from choosing the right model to refining the learning process.

## 0. –ù–µ–±–æ–ª—å—à–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ –≤ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
–î–∞–≤–∞–π—Ç–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ –≤–∫—Ä–∞—Ç—Ü–µ —Ä–∞–∑–±–µ—Ä–µ–º, –∫–∞–∫ —á–∏—Å–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ –∏ –∫–∞–∫–∏–µ —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç. –ù–∞–º —ç—Ç–æ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–æ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.

### Int16/Int8/Int4
–°–∞–º—ã–µ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã–µ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã. –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π - \\([-2^{n-1}, 2^{n-1} - 1]\\)

–°—Ö–µ–º–∞—Ç–∏—á–Ω–æ –±–∏—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ Int16 –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Ç–∞–∫: 1 –±–∏—Ç –∑–Ω–∞–∫–∞ –∏ 15 –±–∏—Ç –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/WY7E6uMR73aigsfcsCq8H.png)

–ß–µ–º –±–æ–ª—å—à–µ –±–∏—Ç–æ–≤, —Ç–µ–º —Ç–æ—á–Ω–µ–µ –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π.

### Float32
–ó–¥–µ—Å—å –±–∏—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫: 1 –±–∏—Ç –∑–Ω–∞–∫–∞, 8 ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã, 23 ‚Äî –º–∞–Ω—Ç–∏—Å—Å—ã.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/KSS-oRLsPUnQ9Vypo7UZp.png)

–§–æ—Ä–º—É–ª–∞:
$$ v = (-1)^{\text{sign}} \cdot 2^{E-127} \cdot \left(1 + \sum_{i=1}^{23} b_{23-i}2^{-i}\right) $$

–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤: —á–µ–º –±–æ–ª—å—à–µ –±–∏—Ç–æ–≤ –≤—ã–¥–µ–ª–µ–Ω–æ –ø–æ–¥ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—É, —Ç–µ–º –±–æ–ª—å—à–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å. –ë–∏—Ç—ã, –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–ª—è –º–∞–Ω—Ç–∏—Å—Å—ã, –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ —Ç–æ—á–Ω–æ—Å—Ç—å, —Å –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ.

### Float16
–ë–∏—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: 1 –±–∏—Ç –∑–Ω–∞–∫–∞, 5 ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –∏ 10 ‚Äî –º–∞–Ω—Ç–∏—Å—Å—ã.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/bX17lqakEY903HrSCZF-c.png)

–ì–ª–∞–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ float16 ‚Äî –º–∞–ª–µ–Ω—å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ 65504, –∏–∑-–∑–∞ —á–µ–≥–æ —Ç–µ–Ω–∑–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ª–µ–≥–∫–æ –ø–µ—Ä–µ–ø–æ–ª–Ω—è—é—Ç—Å—è.

### Bfloat16, –∏–ª–∏ brain float
–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π Google Brain. –ú–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é float32. –ë–∏—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–∫–æ–µ: 1 –±–∏—Ç –∑–Ω–∞–∫–∞, 8 ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –∏ 7 ‚Äî –º–∞–Ω—Ç–∏—Å—Å—ã.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/jeGGZP2DxQfXZZuB72iRD.png)

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —á–∏—Å–ª–æ –±–∏—Ç–æ–≤ –ø–æ–¥ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—É —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º float32. –ó–Ω–∞—á–∏—Ç, bfloat16 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—Ç –∂–µ –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π, –ø—É—Å—Ç—å –∏ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ. –ó–∞—Ç–æ –º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ –æ–ø–∞—Å–∞—Ç—å—Å—è –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–π –≤ –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö.

–î—Ä—É–≥–∞—è –ø—Ä–∏—è—Ç–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å bf16 ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –±—ã—Å—Ç—Ä–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤–æ float32. –ú–∞–≥–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è —Å—Ö–æ–¥–Ω–æ–º—É –±–∏—Ç–æ–≤–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ–∫–∞ —á—Ç–æ –Ω–µ –≤—Å—ë –∂–µ–ª–µ–∑–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —ç—Ç–∏–º —Ç–∏–ø–æ–º (–æ—Å–æ–±–µ–Ω–Ω–æ –º–æ–±–∏–ª—å–Ω–æ–µ).

### TensorFloat32

–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π 19-–±–∏—Ç–Ω—ã–π [—Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö](https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/) –æ—Ç NVidia. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö, –Ω–∞—á–∏–Ω–∞—è —Å NVidia Ampere (A-100). –ë–∏—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: 1 –±–∏—Ç –∑–Ω–∞–∫–∞, 8 ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã, 10 ‚Äî –º–∞–Ω—Ç–∏—Å—Å—ã.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/ha7W9jLH-O1BvMrG5cAQf.png)

–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- —á–∏—Å–ª–æ –±–∏—Ç–æ–≤ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å bfloat16, –∞ –∑–Ω–∞—á–∏—Ç –∏ —Å float32;
- —á–∏—Å–ª–æ –±–∏—Ç–æ–≤ –º–∞–Ω—Ç–∏—Å—Å—ã —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å float16.

–ü–æ–ª—É—á–∏–ª—Å—è –Ω–µ–æ–±—ã—á–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö. –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –ù–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞—Ö NVidia.

### E4M3 –∏ E5M2
–ù–æ–≤—ã–µ 8-–±–∏—Ç–Ω—ã–µ float. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã NVidia, ARM –∏ Intel –≤ —Å—Ç–∞—Ç—å–µ [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433).
–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö 8-–±–∏—Ç–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è:
- E4M3: 1 –±–∏—Ç –∑–Ω–∞–∫–∞, 4 ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã, 3 ‚Äî –º–∞–Ω—Ç–∏—Å—Å—ã
- E5M2: 1 –±–∏—Ç –∑–Ω–∞–∫–∞, 5 ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã, 2 ‚Äî –º–∞–Ω—Ç–∏—Å—Å—ã

–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∏ ¬´–∫–∞—Ä—Ç–∏–Ω–æ—á–Ω—ã–µ¬ª —Å–µ—Ç–∏ –º–æ–∂–Ω–æ —É—Å–ø–µ—à–Ω–æ –∏–Ω—Ñ–µ—Ä–∏—Ç—å –∏ –¥–∞–∂–µ –æ–±—É—á–∞—Ç—å –Ω–∞ —Ç–∞–∫–∏—Ö —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –ñ–¥—ë–º —à–∏—Ä–æ–∫–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ –∂–µ–ª–µ–∑–µ. –°—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –±–æ–ª–µ–µ —Ä–∞–¥–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–∏ 4-–±–∏—Ç–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: E2M1 –∏ E3M0.

## [1. Where Did All the Memory Go?](https://arxiv.org/abs/1910.02054)

Let‚Äôs examine the memory consumption of the current training system. For example, a 1.5B parameter GPT-2 model requires 3GB (1.5B * 16bit) of memory for its weights (or parameters) in 16-bit precision, yet, it cannot be trained on a single GPU with 32GB memory using Tensorflow or PyTorch. One may wonder where all the memory goes. During model training, most of the memory is consumed by *model states*, i.e., tensors comprising of optimizer states, gradients, and parameters. Besides these model states, the rest of the memory is consumed by activations, temporary buffers and fragmented memory which we call *residual states*. We look at the memory consumption from both in details. 

### 1.1 Model States: Optimizer States, Gradients and Parameters

Majority of the device memory is consumed by model states during training. Consider for instance, [Adam](https://arxiv.org/abs/1412.6980), one of the most popular optimizers for DL training. Adam requires storing two optimizer states, 1) the time averaged momentum and 2) variance of the gradients to compute the updates.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/qvSg5entCT4Uk4BAOGsKW.png)

Therefore, to train a model with Adam, there has to be enough memory to hold a copy of both the momentum and variance of the gradients. In addition, there needs to be enough memory to store the gradients and the weights themselves. Of these three types of the parameter-related tensors, the optimizer states usually consume the most memory, specially when mixed-precision training is applied.

**Mixed-Precision Training** The state-of-the-art approach to train large models on the current generation of NVIDIA GPUs is via [mixed precision training](https://arxiv.org/abs/1710.03740), where parameters and activations are stored as fp16, enabling the use of the high throughput tensor core units on these GPUs. During mixed-precision training, both the forward and backward propagation are performed using fp16 weights and activations. However, to effectively compute and apply the updates at the end of the backward propagation, the mixed-precision optimizer keeps an fp32 copy of the parameters as well as an fp32 copy of all the other otimizer states.

![image/gif](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/QmMbZaLmppCKaIo0fWtHT.gif)

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/h0p-IciIv8sVY1I3l_wUL.png)

Let‚Äôs take Adam as a concrete example. Mixed precision training of a model with Œ¶ parameters using Adam requires enough memory to hold an fp16 copy of the parameters and the gradients, with memory requirements of 2Œ¶ and 2Œ¶ bytes respectively. In addition, it needs to hold the optimizer states: an fp32 copy of the parameters, momentum and variance, with memory requirements of 4Œ¶, 4Œ¶, and 4Œ¶ bytes, respectively.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/zlD-T_HtyeSKEY_zhqLix.png)

In total, this results 16Œ¶ bytes of memory requirement. For a model such as GPT-2 with 1.5 Billion parameters, this leads to a memory requirement of at least 24 GB, which is significantly higher than the meager 3 GB of memory required to hold the fp16 parameters alone.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/3DuZdDRbhLK46MfVhKJJX.png)

### 1.2 Residual Memory Consumption
**Activations** can take up a significant amount of memory during training. As a concrete example, the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of 32 requires about 60 GB of memory. 

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/iuytzmuBVVrIPUb72hj3s.png)

![image/gif](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/GgNu3RrWs0ls9AX3jFUmk.gif)

The activation memory of a transformer-based model is proportional to the number of *transformer layers* √ó *hidden dimensions* √ó *sequence length* √ó *batch size*. 

[**Activation checkpointing**](https://arxiv.org/abs/1604.06174) (or gradient checkpointing) is a common approach to reduce the activation memory by approximately the square root of the total activations at the expense of 33% re-computation overhead. This would reduce the activation memory consumption of this model from 60 GB to about 8 GB. 

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/BsRo4b2J31zUFr-KMcs_n.png)

![image/gif](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/ThPtDpjoHZ0GLBsRBfxAR.gif)

Despite the significant reduction, the activation memory can grow quite large for bigger models even with activation checkpointing. For example, a GPT-like model with 100 billion parameters requires around 60 GB of memory for batch size 32, even when using activation checkpointing.

**Temporary buffers** used for storing intermediate results consumes non-trivial amount of memory for large models. Operations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput. For example, the bandwidth of all-reduce across devices improves with large message sizes. While the gradient themselves are usually stored as fp16 tensors, the fused buffer can be an fp32 tensor depending on the operation. When the size of the model is large, these temporary buffer sizes are non-trivial. For example, for a model with 1.5B parameters, a flattened fp32 buffer would required 6 GB of memory

**Memory Fragmentation**: So far we have discussed the actual memory consumption during training. Additionally, it is possible to run out of usable memory even when there is plenty of available memory. This can happen with memory fragmentation. A request for a memory will fail if there isn‚Äôt enough contiguous memory to satisfy it, even if the total available memory is larger than requested. We observe significant memory fragmentation when training very large models, resulting in out of memory issue with over 30% of memory still available in some extreme cases.

## 2. Quantization
Quantization is a procedure for compressing NN models by representing parameters and/or activations with a lower-bit representation such as 8-bit or 4-bit integer, instead of 32-bit or 16-bit floating point.

C–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ –ª–∏–Ω–µ–π–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ –Ω–∞ —Å–∞–º–æ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ–º –∏ –¥–æ–∫–∞–∑–∞–≤—à–µ–º —Å–≤–æ—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–µ.

### 2.1 –ù–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –∏ –°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è 
–í–∑–≥–ª—è–Ω–µ–º —Å–Ω–∞—á–∞–ª–∞ –Ω–∞ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–∏:

**–ù–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è**:
![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/E1qaUh4uRmMXMfmxPlSiu.png)

**–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è**:
![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/QMMum7lBhmZlPj-BCANn8.png)

–¢–æ –µ—Å—Ç—å, –º—ã –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —á–∏—Å–µ–ª –≤ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π. –°–∞–º –ø—Ä–æ—Ü–µ—Å—Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–∂–Ω–æ –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–∫:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/JwT-g6-J31Hce_xylvAxD.png)

–ì–¥–µ **S** –∏ **Z** ‚Äî —ç—Ç–æ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏, —Ç–æ –µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ. **S** - scale, –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –º–∞—Å—à—Ç–∞–± –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. **Z** - zero point, c–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω—É–ª–µ–≤–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é. 
- **–ù–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è**
  - \\(S = \frac {r_{max}-r_ {min}}{q_{max}-q_{min}} \\)
  - \\(Z = \left[q_{min} - \frac{r_{min}}{S}\right]\\)
  - \\(X_{quantized} = \left[\frac{X}{S} + Z\right]\\)
  - \\(X_{dequantized} = S(X_{quantized} - Z)\\)

- **–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è**
  - –ì—Ä–∞–Ω–∏—Ü—ã –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∫–∞–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–æ –º–æ–¥—É–ª—é –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
  - \\(S = \frac{|r|_{max}}{2^{N-1} - 1} \\)
  - \\(Z = 0\\)
  - \\(X_{quantized} = \left[\frac{X}{S}\right]\\)
  - \\(X_{dequantized} = SX_{quantized}\\)
  - –ß—Ç–æ–±—ã —Ç–∏–ø –ø–æ–ª—É—á–∏–ª—Å—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º, –Ω—É–∂–Ω–æ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –æ–¥–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–∏–ø–µ –¥–∞–Ω–Ω—ã—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–∏–∞–ø–∞–∑–æ–Ω signed int8: [-128, 127] –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ [-127, 127]

–≥–¥–µ \\([  ]\\) - –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ.

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ ‚Äî –æ–Ω–∞ —É–º–µ–µ—Ç —Ç–æ—á–Ω–µ–µ –∏ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç –∑–∞ —Å—á—ë—Ç –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏. –ü—Ä–∏ —Ç–∞–∫–æ–º –ø–æ–¥—Ö–æ–¥–µ –Ω–µ –Ω—É–∂–Ω–æ –¥—É–º–∞—Ç—å –æ —Ö—Ä–∞–Ω–µ–Ω–∏–∏ zero-point, –∞ –¥–ª—è –¥–µ–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–º–Ω–æ–∂–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É.

–ü—Ä–∏–º–µ—Ä:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/6qS8qC8WTNdZFcnV16ufS.png)

–ì–æ—Ç–æ–≤–æ. –ù–∞ –≤—ã—Ö–æ–¥–µ –º—ã –ø–æ–ª—É—á–∏–ª–∏ 8-–±–∏—Ç–Ω—ã–π —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ 23,5. –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –º–µ–Ω—å—à–∏–π –æ–±—ä—ë–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å—Å—è –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É 32-–±–∏—Ç–Ω–æ–º—É –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é —Å –ø–æ—Ç–µ—Ä–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.

### 2.2 –ß—Ç–æ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å?

–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ ‚Äî –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏. –ù–∏–∫–∞–∫–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –Ω–µ –Ω—É–∂–Ω—ã, –ø—Ä–æ—Å—Ç–æ –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å —Ñ–æ—Ä–º—É–ª–∞–º–∏.

–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã —Å–ª–æ—ë–≤ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å, –∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Ç–µ–Ω–∑–æ—Ä–∞—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π. –ö–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å? –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–Ω—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É. –° –ø–æ–º–æ—â—å—é —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –Ω–∞–∑—ã–≤–∞—é—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π.

–ê –ø—Ä–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–≤–∞–Ω—Ç–∏–∑—É—é—Ç—Å—è –Ω–∞ inference. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –º–æ–∂–µ—Ç –¥–∞—Ç—å –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ —Å –Ω–∏–º –≤–æ–∑–º–æ–∂–Ω—ã —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏: –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ inference –∏—Å–∫–∞—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø—Ä–∏–¥—ë—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –º–µ—Ç–æ–¥ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º, –∑–∞—Ç–æ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞—é—Ç—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏.

### 2.3 –ö–æ–≥–¥–∞ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å?

–ì–æ—Ç–æ–≤–∏—Ç—å —Å–µ—Ç—å –∫ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è Quantize-Aware. –î–ª—è —ç—Ç–æ–≥–æ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≤—Å—Ç—Ä–∞–∏–≤–∞—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –∏ –≤ —Ö–æ–¥–µ –æ–±—É—á–µ–Ω–∏—è —ç–º—É–ª–∏—Ä—É—é—Ç –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–π inference.

Quantize-Aware-–æ–±—É—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–µ –∏ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –Ω–æ –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å, ¬´–ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–∞—è¬ª –∫ —Ä–∞–±–æ—Ç–µ —Å –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è.

–í —Å–ª—É—á–∞–µ Post Training –∫–≤–∞–Ω—Ç–∏–∑—É—é—Ç —É–∂–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–Ω—É—é —Å–µ—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≥–æ–Ω—è—é—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, —Å–æ–±–∏—Ä–∞—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–Ω–∑–æ—Ä–∞–º –∏ –ø–æ—Ç–æ–º –∫–≤–∞–Ω—Ç–∏–∑—É—é—Ç. –ï—Å–ª–∏ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞, –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω—É–∂–Ω—ã, —Ç–∞–∫ –∫–∞–∫ –≤—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —É–∂–µ –µ—Å—Ç—å –≤ —Ç–µ–Ω–∑–æ—Ä–∞—Ö. –≠—Ç–æ—Ç —Å–ø–æ—Å–æ–± –ø—Ä–æ—â–µ –∏ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º Quantize-Aware, –Ω–æ —É—Å—Ç—É–ø–∞–µ—Ç –µ–º—É –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏.

### 2.4 –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å

–ù–µ–π—Ä–æ—Å–µ—Ç—å –º–æ–∂–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å —Å —Ä–∞–∑–Ω–æ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å—é. –°–∞–º—ã–π –ø–ª–æ—Ö–æ–π —Å–ø–æ—Å–æ–± ‚Äî –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å —Å—Ä–∞–∑—É –≤—Å—é —Å–µ—Ç—å –∑–∞ —Ä–∞–∑. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —É –≤–∞—Å –ø–æ–ª—É—á–∏—Ç—Å—è –æ–¥–Ω–∞ –æ–±—â–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ S –Ω–∞ –≤—Å—é –º–æ–¥–µ–ª—å. –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞–∫–∏—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –æ–∫–∞–∂–µ—Ç—Å—è –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–º.

–ú–æ–∂–Ω–æ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Äî —Ç–æ–≥–¥–∞ –∫–∞–∂–¥—ã–π —Ç–µ–Ω–∑–æ—Ä –ø–æ–ª—É—á–∏—Ç —Å–≤–æ–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã. –ê –º–æ–∂–Ω–æ –ø–æ–π—Ç–∏ –¥–∞–ª—å—à–µ –∏ –≤ –∫–∞–∂–¥–æ–º —Ç–µ–Ω–∑–æ—Ä–µ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ —Å—Ç–æ–ª–±—Ü—ã. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —É –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ (—Å—Ç–æ–ª–±—Ü–∞) –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –±—É–¥–µ—Ç —Å–≤–æ—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞. –ò—Ö –ø—Ä–∏–¥—ë—Ç—Å—è –≥–¥–µ-—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å, –∑–∞—Ç–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –±—É–¥—É—Ç —Ç–æ—á–Ω–µ–µ.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/xbY-NhjLaCY88RPi5PNee.png)

–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –Ω–∞—Ä–µ–∑–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –±–ª–æ–∫–∏ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ ‚Äî —Ç–∞–∫ –ø–æ–ª—É—á–∏—Ç—Å—è –µ—â—ë —Ç–æ—á–Ω–µ–µ. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ –≤ –º–∞—Ç—Ä–∏—Ü–∞—Ö, –æ —á—ë–º –º—ã –∏ –ø–æ–≥–æ–≤–æ—Ä–∏–º –¥–∞–ª—å—à–µ.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/S1SvT4tE5OEVvTskumu3c.png)

–ò—Ç–∞–∫, —á–µ–º –º–µ–Ω—å—à–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å, —Ç–µ–º –º–µ–Ω—å—à–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç –Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å, –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç ‚Äî —á–µ–º –≤—ã—à–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å, —Ç–µ–º –±–ª–∏–∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∫ –∏—Å—Ö–æ–¥–Ω—ã–º.

### 2.5 –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö

–í –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –æ–±—ã—á–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–≤–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö:

- **Quantized type** ‚Äî –≤ —ç—Ç–æ–º —Ç–∏–ø–µ —Ö—Ä–∞–Ω—è—Ç —Ç–µ–Ω–∑–æ—Ä—ã
- **Computation type** ‚Äî –≤ —ç—Ç–æ–º —Ç–∏–ø–µ –ø—Ä–æ–≤–æ–¥—è—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.

–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —ç—Ç–∏ –¥–≤–∞ —Ç–∏–ø–∞ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–æ–≤–ø–∞–¥–∞—é—Ç. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–∞—à–µ –∂–µ–ª–µ–∑–æ –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —Ö–∏—Ç—Ä–æ–º quantized type. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∫–µ—Ä–Ω–µ–ª–æ–≤ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –ø–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–∏–ø –º–æ–∂–µ—Ç –ø—Ä–æ—Å—Ç–æ –Ω–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å. –í —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –ø–µ—Ä–µ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏ –º–∞—Ç—Ä–∏—Ü—É –Ω—É–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ computation type. –¢–∞–∫–∂–µ computation type –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –≤ –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö, —Ç–∞–∫ –∫–∞–∫ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏–µ 8-–±–∏—Ç–Ω—ã—Ö —á–∏—Å–µ–ª –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ –ø—Ä–∏–≤–µ–¥—ë—Ç –∫ –≤—ã—Ö–æ–¥—É –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã —Ç–∏–ø–∞.

### 2.6 –ü—Ä–æ–±–ª–µ–º–∞ –≤—ã–±—Ä–æ—Å–æ–≤
–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/AHIUqmayD-GX7PpEPirpg.png)

–ß—Ç–æ –ø–æ–ª—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –≤–æ –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ–ø–∞–¥—ë—Ç –≤—ã–±—Ä–æ—Å?

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/M-NJYW1SuSDNU4qiHa5Dv.png)

–í–µ—Å–∞ ¬´—Å–∫–ª–µ–∏–ª–∏—Å—å¬ª –≤ —É–∑–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –∏ —Å—Ç–∞–ª–∏ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã. –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –ø–æ—Ç–µ—Ä—è–Ω–æ. –¢–∞–∫ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±—Ä–æ—Å –∏—Å–ø–æ—Ä—Ç–∏–ª –≤—Å—é –º–∞—Ç—Ä–∏—Ü—É.

–ö–æ–≥–¥–∞ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –±–æ–ª—å—à–µ –∏ –±–æ–ª—å—à–µ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ—Å—Ç–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å. –ü—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –≥—Ä–∞–Ω–∏—Ü—ã –≤ 6,7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ [—Ç–µ—Ä—è—é—Ç –≤—Å—ë –∫–∞—á–µ—Å—Ç–≤–æ](https://arxiv.org/abs/2208.07339). –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —ç—Ç–æ –∏–∑-–∑–∞ —Ä–∞—Å—Ç—É—â–µ–≥–æ —á–∏—Å–ª–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –º–∞—Ç—Ä–∏—Ü–∞—Ö

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/juzSha_nIdR3znammiwgO.png)









#### 1.1. Mixed Precision
Before diving into Mixed Precision and related topics, it‚Äôs crucial to understand what contributes to memory consumption during model training. A model consists of parameters, each represented as a real number stored in the computer's memory. Typically, these real numbers are stored in the float32 format, which requires 32 bits per number.

To put this in perspective, let's calculate the memory needed to load a model like Llama 70B. This model has 70 billion parameters, so it would require approximately 260.77 GB of memory (32 * 70,000,000,000 bits ‚âà 260.77 GB). But that‚Äôs just the start. During training, we also need to store gradients for each parameter, which adds another 260 GB. Additionally, storing the first moment (inertia) and the second moment (adaptive learning rate) of optimizer like Adam for each parameter requires another 260 GB each.

In total, just to train a model with 70 billion parameters, you‚Äôd need approximately 1040 GB of GPU memory. And this doesn‚Äôt even account for the memory needed for activations, which are related to the batch size, data size (e.g., sequence length), and model architecture. Although we won‚Äôt include activations in our future calculations to maintain generality, it's worth noting that they occupy a comparable amount of memory to the model‚Äôs weights.

Having established the memory requirements for training in float32, let's explore how Mixed Precision works.

The key idea behind Mixed Precision is whether we can achieve sufficient accuracy by training models in float16, thereby reducing memory consumption and computation time by half. However, we can't simply convert all computations to float16 as this would lead to [numerical instability](https://arxiv.org/abs/2010.06192v1)

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/W_iFlLm64CUEtm3MNzTKT.png)

**Mixed Precision** training is a technique that enables the use of float16 without causing the model training to diverge. It involves three main strategies:
- **Maintaining two copies of the weights matrix**: A ‚Äúmaster copy‚Äù in float32 and a float16 copy. Gradient updates are calculated using the float16 matrix but applied to the float32 matrix, making the gradient update process safer.
- **Selective precision**: Different operations accumulate errors at different rates. Some operations are always safe in float16, while others are reliable only in float32. Therefore, instead of running the entire neural network in float16, some parts are run in float16 and others in float32. This mixture of data types is what gives the technique its name‚Äî"mixed precision."
- **Loss scaling**: Since float16 has a limited range, loss scaling is used to prevent underflow. However, with the advent of bfloat16 in NVIDIA GPUs starting from the Ampere series, loss scaling is no longer necessary because bfloat16 has a similar range to float32.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/vDlps5gqM3khBs3ADMDa5.png)

Now, let‚Äôs recalculate the memory requirements for training in Mixed Precision.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/dSfhohT9rwPS6xafWunP4.png)

Each parameter now requires 16 bytes, so training Llama 70B would still require approximately 1040 GB. You might wonder why the memory usage remains the same as in float32. The reason is that while we use 2 bytes (16 bits) for weights and gradients in float16, we also store a copy of the weights in float32, adding 4 bytes per parameter. But. there might be the major saving come from reduced activation memory.

Also the significant advantage of Mixed Precision lies in computation speed‚Äîmost calculations are now done in float16, which considerably speeds up the training process.

#### 1.2. PEFT (Parameter-Efficient Fine-Tuning)
PEFT is a family of methods designed to efficiently adapt large-scale models by training only a small subset of parameters. These methods significantly reduce computational costs and memory requirements while maintaining quality comparable to full fine-tuning.

One of the most popular and effective PEFT methods is [LoRa](https://arxiv.org/abs/2106.09685).

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/Q0d07jIXg43H4IAEgAJJN.png)

To understand the illustration, let's delve into the fundamental observation that makes this method effective:
>A neural network contains many dense layers which perform matrix multiplication. The weight
matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al.
(2020) shows that the pre-trained language models have a low ‚Äúinstrisic dimension‚Äù and can still
learn efficiently despite a random projection to a smaller subspace

This means that while training for a broad, complex task, the weight matrices in a neural network have full rank, which minimizes redundancy. However, when fine-tuning this universal model for a specialized task, not all the knowledge from the original model is necessary. Therefore, only a small fraction of the parameters needs to be trained. In simpler terms, the weight matrices can be represented by smaller matrices with fewer parameters. Thus, during full fine-tuning, the weight matrices can be considered low-rank, indicating that full fine-tuning involves some degree of redundancy.

>Inspired by this, we hypothesize the updates to the weights also have a low ‚Äúintrinsic rank‚Äù during adaptation.

Given that low-rank weight matrices suffice for full fine-tuning on a downstream task, it's reasonable to assume that the gradient updates themselves can be represented by low-rank matrices. 

>For a pre-trained weight matrix \\(W_0 \in \mathbb{R}^{d\times d}\\), we constrain its update by representing the latter with a low-rank decomposition \\(W_0 + \Delta W = W_0 + BA\\), where \\(B \in \mathbb{R}^{d\times r}\\), \\(A \in \mathbb{R}^{r\times k}\\), and the rank \\(r \ll d\\). During training, \\(W_0\\) is frozen and does not receive gradient updates, while \\(A\\) and \\(B\\) contain trainable parameters. Note both \\(W_0\\) and \\(\Delta W = BA\\) are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For \\(h = W_0x\\) our modified forward pass yields:
$$ h = W_0x + \Delta W x = W_0x + BAx $$

In essence, we freeze the original model, insert low-rank adapters under the relevant weight matrices, and train these adapters to simulate the updates that would normally come from gradients. With these concepts and the formulas above, you should now understand the illustration provided.

Where do the memory and computation optimizations come from? Since the baseline model is frozen, we don‚Äôt store gradients or optimizer moments for it, and we avoid unnecessary computations. Essentially, with a few caveats, we now only need to perform inference on the baseline model, which, in the case of large models, still requires significant hardware resources. However, the trainable parameters in these adapters typically constitute less than 1% of the total parameters of the original model.

#### 1.3. Quantization –∏ QLoRa

You might wonder, "*Since fp16 works so well, can we reduce the precision of the numbers even further‚Äîto 8 bits or even 4?*" This is the essence of quantization. However, simply downcasting to 8 or 4 bits would make the computations highly unstable, especially during training.

Quantization aims to reduce memory usage with minimal loss of accuracy. While many types of quantization exist, I'll focus on the most basic and commonly used method.

Generally, quantization is applied only during inference because training with such low-precision numbers is highly unstable. However, it's possible to train adapters on top of a quantized model ‚Äî a concept I'll explore later.

So, how does basic quantization work? Let's take a look at the figure:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/zI_ZTgUutnP1YW70r5D2q.png)

In simple terms, we linearly scale the parameters in the matrices from fp32 to int8 (or int4), while keeping the constants needed for reverse dequantization. This allows us to create a highly compressed model with significantly reduced memory requirements.

During inference, as the computation moves through the layers, the required parameters are dequantized, the necessary computations are performed (e.g., in fp16), and then the parameters are re-quantized before moving to the next layer.
I am simplifying things a lot, because the analysis of quantization techniques can be a separate article, but I think you have understood the main idea.

And it turns out that after such quantization, the quality of language models on inference drops quite insignificantly. This is also due to the fact that in language modeling we do not care so much about exact probabilities in predicting the next token. Basically, we just sample from the dictionary distribution predicted by the model

You might ask, "*Can this idea be applied to training?*" While training directly on a quantized model is not feasible, training adapters on top of a quantized model is possible, which is the basis of the brilliant [QLoRA](https://arxiv.org/abs/2305.14314) 

That is, there will only be a small change in the illustration about LoRa:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/eJkmp305QaBin8vtzJf77.png)

Let‚Äôs delve deeper into the QLoRa method:

First, a bit more about quantization:
>**Block-wise k-bit Quantization.** Quantization is the process of discretizing an input from a representation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum
of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range [‚àí127, 127]:
$$ \mathbf{X}^{\text{Int8}} = \text{round}\left(\frac{127}{\text{absmax}(\mathbf{X}^{\text{FP32}})} \mathbf{X}^{\text{FP32}}\right) = \text{round}(c^{\text{FP32}} \cdot \mathbf{X}^{\text{FP32}}),$$
where c is the *quantization constant* or *quantization scale*. Dequantization is the inverse:
$$ \text{dequant}(c^{\text{FP32}}, \mathbf{X}^{\text{Int8}}) = \frac{\mathbf{X}^{\text{Int8}}}{c^{\text{FP32}}} = \mathbf{X}^{\text{FP32}} $$
The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins‚Äîcertain bit combinations‚Äîare not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can be formalized as follows: We chunk the input tensor \\(\mathbf{X} \in \mathbb{R}^{b \times h}\\) into n contiguous blocks of size B by flattening the input tensor and slicing the linear segment into \\(n = (b \times h) / B\\) blocks. We quantize these blocks independently with Equation 1 to create a quantized tensor and n quantization constants \\(c_i\\)

The Block-wise method for outlier avoidance is worth noting here.

The QLoRA authors also proposed two valuable techniques:
- **Double Quantization**: Here, even the quantization constants *c* are quantized, further saving memory.
- **4-bit NormalFloat**: Leveraging the fact that pretrained neural network weights typically have a zero-centered normal distribution, this technique allows for a more informative mapping from fp32 to int4, with higher precision near zero.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/qWvY2qGfbZfOGr8T_rewn.png)

Now, let‚Äôs understand the entire QLoRA process (L1 and L2 in the formulas correspond to B and A in the figure):
> **QLoRA**. Using the components described above, we define QLORA for a single linear layer in the quantized base model with a single LoRA adapter as follows:
$$ \mathbf{Y}^{\text{BF16}} = \mathbf{X}^{\text{BF16}} \text{doubleDequant}(c_1^{\text{FP32}}, c_2^{k\text{-bit}}, \mathbf{W}^{\text{NF4}}) + \mathbf{X}^{\text{BF16}} \mathbf{L}_1^{\text{BF16}} \mathbf{L}_2^{\text{BF16}} $$
where doubleDequant(¬∑) is defined as:
$$ \text{doubleDequant}(c_1^{\text{FP32}}, c_2^{k\text{-bit}}, \mathbf{W}^{k\text{-bit}}) = \text{dequant}(\text{dequant}(c_1^{\text{FP32}}, c_2^{k\text{-bit}}), \mathbf{W}^{4\text{bit}}) = \mathbf{W}^{\text{BF16}} $$
We use NF4 for \\(\mathbf{W}\\) and FP8 for \\(c_2\\). We use a blocksize of 64 for \\(\mathbf{W}\\) for higher quantization precision and a blocksize of 256 for \\(c_2\\) to conserve memory.
>
>For parameter updates only the gradient with respect to the error for the adapters weights \\(\frac{\partial E}{\partial \mathbf{L}_i}\\) are needed, and not for 4-bit weights \\(\frac{\partial E}{\partial \mathbf{W}}\\). However, the calculation of \\(\frac{\partial E}{\partial \mathbf{L}_i}\\) entails the calculation of \\(\frac{\partial \mathbf{X}}{\partial \mathbf{W}}\\) which proceeds via first equation with dequantization from storage \\(\mathbf{W}^{\text{NF4}}\\) to computation data type \\(\mathbf{W}^{\text{BF16}}\\) to calculate the derivative \\(\frac{\partial \mathbf{X}}{\partial \mathbf{W}}\\) in BFloat16 precision.
>
>To summarize, QLORA has one storage data type (usually 4-bit NormalFloat) and a computation data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type to perform the forward and backward pass, but we only compute weight gradients for the LoRA parameters which use 16-bit BrainFloat.

Let‚Äôs calculate the memory requirements for a model like Llama 70B using QLoRa: 
1. Quantizing the model to 4-bit reduces the size to 65GB.
2. Adding LoRa adapters, which, let's say, occupy about 0.25% of the original model‚Äôs parameters (175 million parameters), and training these in Mixed Precision requires approximately 2.6GB (175M * 16 bytes).
3. Of course, activations for such a large model will still require significant memory, but we'll soon discuss how gradient checkpointing can help mitigate this.

In summary, we‚Äôve reduced the memory requirement from 1040GB to 68GB! (Using LoRa without quantization, the frozen base model in 16-bit would take 260GB, plus the same 2.6GB for training adapters).

#### 1.4. Gradient Checkpointing

Get ready for some cool visualizations!

First, let's examine the computational graph for the forward and backward passes. Don't worry if it looks complex at first‚Äîthere are simpler animations below that will make everything clear.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/nULvbrYGgR53_D1-fZzC_.png)

You might be wondering, what exactly are the "scary activations" we mentioned earlier? Let's break down what we aim to achieve in the learning process:
1. Our goal is to improve the model, i.e., to reduce the error.
2. We reduce this error by adjusting the model's weights.
3. To adjust the weights, we need to calculate the gradient of the error function with respect to these weights.
4. We achieve this by using the chain rule to propagate the gradient from the error back through the network to the weights during the backward pass.
5. To do this, we at least need the error value itself.
6. This means we first need to perform a forward pass‚Äîrunning the input data through the entire model to generate a prediction.
7. Most critically, to compute the gradients, we need to account for all the intermediate computations and the dependent terms involved in generating the prediction. These are the activations that we store in memory from the entire forward pass so we can use them during the backward pass (represented by the gray and black sections in the picture).

This process can be schematically represented as follows:

![image/gif](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/SwMyFsZSETbZeksAa5bCO.gif)

Now, let's think about how we can reduce memory consumption. One idea might be to recalculate each activation during the backward pass only when we need it:

![image/gif](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/mHYCcFuL2Bjcm774X4rVa.gif)

However, this approach leads to too many recalculations, which negatively impacts training time. This is a classic example of the Time-Memory Trade-Off in programming.
So, what's a good compromise? Here's a solution:
1. Select several "checkpoints" along the path of the forward pass and save only those.
2. During the backward pass, instead of recalculating all activations from the start, we only need to recalculate starting from the nearest checkpoint to the left.

![image/gif](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/yu3lvV-c2WhXmGBg-pW0n.gif)

This method significantly reduces memory consumption, though it does come at the cost of increased training time.

#### 1.5. Flash Attention

Scaling the transformer architecture is heavily bottlenecked by the self-attention mechanism, which has quadratic time and memory complexity. Recent developments in accelerator hardware mainly focus on enhancing compute capacities and not memory and transferring data between hardware. This results in attention operation having a memory bottleneck.

Standard attention mechanism uses High Bandwidth Memory (HBM) to store, read and write keys, queries and values. HBM is large in memory, but slow in processing, meanwhile SRAM is smaller in memory, but faster in operations. In the standard attention implementation, the cost of loading and writing keys, queries, and values from HBM is high. It loads keys, queries, and values from HBM to GPU on-chip SRAM, performs a single step of the attention mechanism, writes it back to HBM, and repeats this for every single attention step.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/g4EnH54JxS_ZFvNgtVNce.png)

[**FlashAttention**](https://tridao.me/publications/flash3/flash3.pdf) is an algorithm that reorders the attention computation and leverages tiling and recomputation to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It uses tiling to load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads/writes, which brings 2-4x wallclock time speedup.

Diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation.
![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/_Xf7ZPpoX6o-17ARq6B4E.png)

For FP16:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/-VbVssfm8mkToIrFxJgWk.png)

We have come a long way. Of the hard stuff, only one topic remains to be dealt with - distributed computing. Before them, let's briefly discuss a few more small methods that can slightly improve your performance

#### 1.6. Gradient Accumulation

**Gradient accumulation** is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.

For instance, if the gradient accumulation factor is set to 2, the process works as follows: We first calculate the gradient on one batch, which gives us a direction on the [loss function landscape](https://losslandscape.com/). Instead of updating the model weights immediately, we calculate another gradient from the next batch, obtaining a potentially different direction. By adding these two gradients together, we find a more accurate path in the loss landscape. To ensure the final update step is properly scaled, we divide the accumulated gradient by the number of batches, preventing any artificial inflation of the step size.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/660710b03ef451aa2bab8971/UJ2nzxFp7EUquy61gDnBZ.png)

This technique is particularly useful when only small batch sizes can fit into memory, which might otherwise lead to overly noisy updates and less stable training.

#### 1.7. 8-bit optimizers

–ü–æ–º–Ω–∏—Ç–µ, –∫–∞–∫ –º–Ω–æ–≥–æ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –ø–∞–º—è—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä? –î–∞–≤–∞–π—Ç–µ —á—É—Ç—å –≥–ª—É–±–∂–µ –ø–æ–π–º–µ–º, –ø–æ—á–µ–º—É. –°–Ω–∞—á–∞–ª–∞ –≤—Å–ø–æ–º–Ω–∏–º —Ñ–æ—Ä–º—É–ª—É –ø—Ä–æ—Å—Ç–µ–π—à–µ–≥–æ SGD –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (x - —ç—Ç–æ –≤–µ—Å–∞):
$$ x_{k+1} = x_k - \alpha \nabla f(x_k) $$

–ö–∞–∫ –≤–∏–¥–∏–º, –∑–¥–µ—Å—å –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤–µ—Å–∞–º. –ù–æ —Ç–∞–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä 

$$ v_{k+1} = \beta_1 v_k + (1 - \beta_1) \nabla f(x_k) $$
$$ G_{k+1} = \beta_2 G_k + (1 - \beta_2) (\nabla f(x_k))^2 $$
$$ x_{k+1} = x_k - \frac{\alpha}{\sqrt{G_{k+1} + \varepsilon}} v_{k+1} $$
