# LLM Оптимизация обучения. Обзор техник

![image](https://github.com/user-attachments/assets/4e3886a2-f8c7-4a74-919a-96276f9ef4b3)


Обучение больших языковых моделей (LLM) - ресурсоемкий процесс, требующий значительных вычислительных мощностей и времени. Эффективная оптимизация процесса обучения позволяет существенно снизить затраты, ускорить разработку и улучшить качество конечной модели. Данный гайд представляет собой комплексный обзор методов оптимизации, от выбора модели до тонкой настройки процесса обучения.

## Методы оптимизации
### 1. Оптимизация архитектуры и данных
Рассмотрим несколько довольно очевидных вещей.
#### 1.1. Выбор модели
Одну из самых простых оптимизаций, которую вы можете сделать в самом начале, это задаться вопросом: "А нужна ли для моей задачи убер-нейронка на 400B параметров, или достаточные метрики можно будет получить с какой-нибудь 8B Llama?" 
#### 1.2. Качество датасета
Качество датасета зачастую важнее его количества. Хорошо подготовленный датасет, даже в 10 раз меньший, чем случайно собранный, часто дает лучшие результаты. Это правило, конечно, не всеисчерпывающее и, например, в случае с претрейном LLM не будет работать в чистом виде.

С очевидными вещами закончили, переходим к более интересным техникам.
#### 1.3. Sequence packing
Последовательности разной длины не позволяют создавать прямоугольные тензоры. Паддинг - распространённое, но неэффективное решение, занимающее [от 50 до 89%](https://arxiv.org/abs/2107.02027) бесполезных вычислений.

Современные LLM используют более эффективный метод: заполнение всей доступной длины контекста. Длинные документы обрезаются с перекрытием, короткие объединяются.

Возникает проблема: во внимании трансформера оказываются несвязанные документы. Решения:

- Токен-разделитель между документами. (Не убирает полностью проблему с вниманием на разные документы, но дает надежду на то, что модель сама научится не придавать большое значение токенам, за пределами разделителей)

- Кастомная маска внимания (сложнее, требует совместимости с FlashAttention).

![image](https://github.com/user-attachments/assets/44dc3e8b-0fb3-4a54-b4b8-cbe27f53f035)

Также нужно учитывать позиционное кодирование для новых документов. Не уверен, нужно ли учитывать эту поправку при использовании относительных позиций типа RoPE и AliBi.

![image](https://github.com/user-attachments/assets/f8a6869e-eda2-412e-88fa-e89853b94d97)

Отсылаю вас к интересному [посту](https://lweitkamp.github.io/posts/packing/#fn2) на эту тему для дальнейшего углубления.

### 2. Оптимизация памяти и вычислений
#### 2.1. Mixed Precision
Перед тем как погружаться в суть Mixed Precision и следующих тем, нам очень важно разобраться, из чего состоит потребление памяти при обучении моделей. 
Модель состоит из параметров. Каждый параметр - вещественное число, которое нужно хранить в компьютере. Обычно вещественные числа для DL задач хранятся в float32, то есть требуют 32 бита на хранение. 

Хорошо, тогда давайте посчитаем, сколько требуется памяти (а так как модели обычно гоняют на GPU, то видеопамяти) чтобы просто загрузить модель, например, Llama 70B. Это будет 32*70000000000 бит ≈ 260,77 ГБ!! Думаете, это всё? Ха-ха. Для обучения нам еще нужно хранить градиент для каждого параметра, то есть это еще +260гб. А еще у нас какой-нибудь Adam считает считает инерцию(первый момент) для каждого параметра (еще +260гб) и адаптивный lr (второй момент) для каждого параметра (еще +260гб). А ведь мы еще даже не подали на вход модели батч...

То есть, просто чтобы подготовить модель на 70B параметров, нужно ≈ 1040 ГБ GPU. Дальше, чтобы запустить обучение, нужно, чтобы через модель пролез хотя бы батч размера 1. Память, связанная с батчем и его прямым и обратным проходом, называют Активациями. Память под активации, соответственно зависит от размера батча, размера данных (например длины последовательности) и от размера архитектуры модели, поэтому мы в дальнейших вычислениях будем опускать этот момент, чтобы не терять в общности. (хотя активации занимают как минимум сопоставимое с весами количество памяти)

Так, с базовым случаем обучения в fp32 вроде бы разобрались. Давайте разберемся, как работает Mixed Precision

"Может быть, нам хватило бы точности при обучении моделей в fp16? Это снизило бы потребление памяти и время вычислений в 2 раза" В этом вопросе кроется основная идея Mixed Precision. Но, сразу скажу, мы не можем тупо все вычисления перевести в формат с числами в fp16, в таком случае обучение будет [численно нестабильным](https://arxiv.org/abs/2010.06192v1).

![image](https://github.com/user-attachments/assets/5332351c-14f7-4090-95a9-3106982e3775)


Таким образом, **Mixed precision** training is a set of techniques which allows you to use fp16 without causing your model training to diverge. It’s a combination of three different techniques:
- maintain two copies of the weights matrix, a “master copy” in fp32, and a half-precision copy of it in fp16. Gradient updates are calculated using the fp16 matrix but applied to the fp32 matrix. This makes applying the gradient update much safer.
- different vector operations accumulate errors at different rates, so treat them differently. Some operations are always safe in fp16, but others are only reliable in fp32. Instead of running the entire neural network in fp16, run some parts in halves and others in singles. This mixture of dtypes is why this technique is called “mixed precision”.
- еще, для fp16 нужен loss scaling, чтобы не выходить за пределы диапазона fp16. Но с новым типом bfloat16 на nvidia gpu начиная с Ampere серии, этого делать не нужно, так как bfloat16 имеет диапазон как у float32

![image](https://github.com/user-attachments/assets/1c54c924-c3da-4205-8c0e-93bdbc36dae4)

Отлично, давайте теперь пересчитаем, сколько нам нужно памяти для обучения в Mixed Precision.

![image](https://github.com/user-attachments/assets/d43358d9-ef89-4a8d-8574-161d89b2b160)

Получается, на каждый параметр нам нужно 16 байт, таким образом Llama 70B заняла бы 16*70000000000 байт ≈ 1040 ГБ. Да, мы ничего не перепутали, всё те же 1040 гб, как в в случае с fp32. Мы, с одной стороны, стали использовать 2байта (16 бит) для весов и для градиентов, но, с другой стороны, компенсировали обратно это тем, что нам нужно хранить копию весов в fp32, то есть +4 байта. 

Но зато теперь мы Большую часть вычислений проводим в fp16! Это очень сильно ускоряет время вычислений.

#### 2.2. PEFT (Parameter-Efficient Fine-Tuning)
PEFT представляет собой семейство методов, направленных на эффективную адаптацию крупномасштабных моделей путем обучения лишь небольшого подмножества параметров. Эти методы позволяют значительно сократить вычислительные затраты и объем требуемой памяти при сохранении качества, сопоставимого с full fine-tuning.

[LoRa](https://arxiv.org/abs/2106.09685) является одним из наиболее популярных и эффективных методов PEFT.

![image](https://github.com/user-attachments/assets/0007836a-4514-4d8d-88b4-281c11c9c213)

Чтобы понять эту иллюстрацию, обратимся к основному наблюдению, благодаря которому этот метод работает.
>A neural network contains many dense layers which perform matrix multiplication. The weight
matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al.
(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still
learn efficiently despite a random projection to a smaller subspace

То есть, когда мы обучаемся под какую-нибудь масштабную и всестороннюю задачу, матрицы весов имеют полный ранг и, следовательно, не страдают избыточностью. Но если мы решили дообучить эту универсальную модель под какую-нибудь узкоспециализированнную задачу, не требующую всех знаний исходной модели, то достаточно было бы обучать лишь очень малую долю от всех параметров. То есть, грубо говоря, матрицы весов можно было бы представить меньшими матрицыми с меньшим количеством весов. А значит, выражаясь более формально, при full fine-tuning матрицы весов можно назвать низкоранговыми, что тоже говорит об избыточности full fine-tuning.

>Inspired by this, we hypothesize the updates to the weights also have a low “intrinsic rank” during adaptation.

Далее, логично было бы предположить, что раз при full fine-tuning под downstream task достатчно низкоранговых матриц весов, то и обновления, приходящие от градиентов, тоже можно попробовать представить низкоранговыми матрицами. 

>For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d\times d}$, we constrain its update by representing the latter with a low-rank decomposition $W_0 + \Delta W = W_0 + BA$, where $B \in \mathbb{R}^{d\times r}$, $A \in \mathbb{R}^{r\times k}$, and the rank $r \ll d$. During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ contain trainable parameters. Note both $W_0$ and $\Delta W = BA$ are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For $h = W_0x$, our modified forward pass yields:
>
>$$h = W_0x + \Delta Wx = W_0x + BAx \$$

То есть, замораживаем исходную модель, вставляем низкоранговые адаптеры под желаемые матрицы весов в модели, обучаем эти адаптеры иммитировать обновления, которые в обычной ситуации приходили бы от градиентов. Думаю теперь, имея эти раасуждения и вышеприведенные формулы, вы можете наконец понять, что было изображено на той иллюстрации сверху. 

Откуда берется выйгрыш в оптимизации памяти и вычислений? Мы замораживаем исходную модель, и поэтому не храним градиенты и моменты оптимизатора для нее, и не проводим лишних вычисений. По сути, с небольшими оговорками, от базовой модели нам теперь нужен только инференс, что, правда, в случае с гиганскими моделями, тоже требует много железа. А обучаемые параметры адаптеров обычно занимают <1% от количества параметров исходной модели.



