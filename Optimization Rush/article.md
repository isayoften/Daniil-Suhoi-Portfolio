# LLM Оптимизация обучения. Обзор техник

![image](https://github.com/user-attachments/assets/4e3886a2-f8c7-4a74-919a-96276f9ef4b3)


Обучение больших языковых моделей (LLM) - ресурсоемкий процесс, требующий значительных вычислительных мощностей и времени. Эффективная оптимизация процесса обучения позволяет существенно снизить затраты, ускорить разработку и улучшить качество конечной модели. Данный гайд представляет собой комплексный обзор методов оптимизации, от выбора модели до тонкой настройки процесса обучения.

## Методы оптимизации
### 1. Оптимизация архитектуры и данных
Рассмотрим несколько довольно очевидных вещей.
#### 1.1. Выбор модели
Одну из самых простых оптимизаций, которую вы можете сделать в самом начале, это задаться вопросом: "А нужна ли для моей задачи убер-нейронка на 400B параметров, или достаточные метрики можно будет получить с какой-нибудь 8B Llama?" 
#### 1.2. Качество датасета
Качество датасета зачастую важнее его количества. Хорошо подготовленный датасет, даже в 10 раз меньший, чем случайно собранный, часто дает лучшие результаты. Это правило, конечно, не всеисчерпывающее и, например, в случае с претрейном LLM не будет работать в чистом виде.

С очевидными вещами закончили, переходим к более интересным техникам.
#### 1.3. Sequence packing
Последовательности разной длины не позволяют создавать прямоугольные тензоры. Паддинг - распространённое, но неэффективное решение, занимающее [от 50 до 89%](https://arxiv.org/abs/2107.02027) бесполезных вычислений.

Современные LLM используют более эффективный метод: заполнение всей доступной длины контекста. Длинные документы обрезаются с перекрытием, короткие объединяются.

Возникает проблема: во внимании трансформера оказываются несвязанные документы. Решения:

- Токен-разделитель между документами. (Не убирает полностью проблему с вниманием на разные документы, но дает надежду на то, что модель сама научится не придавать большое значение токенам, за пределами разделителей)

- Кастомная маска внимания (сложнее, требует совместимости с FlashAttention).

![image](https://github.com/user-attachments/assets/44dc3e8b-0fb3-4a54-b4b8-cbe27f53f035)

Также нужно учитывать позиционное кодирование для новых документов. Не уверен, нужно ли учитывать эту поправку при использовании относительных позиций типа RoPE и AliBi.

![image](https://github.com/user-attachments/assets/f8a6869e-eda2-412e-88fa-e89853b94d97)

Отсылаю вас к интересному [посту](https://lweitkamp.github.io/posts/packing/#fn2) на эту тему для дальнейшего углубления.

### 2. Оптимизация памяти и вычислений
#### 2.1. PEFT (Parameter-Efficient Fine-Tuning)
PEFT представляет собой семейство методов, направленных на эффективную адаптацию крупномасштабных моделей путем обучения лишь небольшого подмножества параметров. Эти методы позволяют значительно сократить вычислительные затраты и объем требуемой памяти при сохранении качества, сопоставимого с full fine-tuning.

[LoRa](https://arxiv.org/abs/2106.09685) является одним из наиболее популярных и эффективных методов PEFT.

![image](https://github.com/user-attachments/assets/0007836a-4514-4d8d-88b4-281c11c9c213)

Чтобы понять эту иллюстрацию, обратимся к основному наблюдению, благодаря которому этот метод работает.
>A neural network contains many dense layers which perform matrix multiplication. The weight
matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al.
(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still
learn efficiently despite a random projection to a smaller subspace

То есть, когда мы обучаемся под какую-нибудь масштабную и всестороннюю задачу, матрицы весов имеют полный ранг и, следовательно, не страдают избыточностью. Но если мы решили дообучить эту универсальную модель под какую-нибудь узкоспециализированнную задачу, не требующую всех знаний исходной модели, то достаточно было бы обучать лишь очень малую долю от всех параметров. То есть, грубо говоря, матрицы весов можно было бы представить меньшими матрицыми с меньшим количеством весов. А значит, выражаясь более формально, при full fine-tuning матрицы весов можно назвать низкоранговыми, что тоже говорит об избыточности full fine-tuning.

>Inspired by this, we hypothesize the updates to the weights also have a low “intrinsic rank” during adaptation.

Далее, логично было бы предположить, что раз при full fine-tuning под downstream task достатчно низкоранговых матриц весов, то и обновления, приходящие от градиентов, тоже можно попробовать представить низкоранговыми матрицами. 

>For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d\times d}$, we constrain its update by representing the latter with a low-rank decomposition $W_0 + \Delta W = W_0 + BA$, where $B \in \mathbb{R}^{d\times r}$, $A \in \mathbb{R}^{r\times k}$, and the rank $r \ll d$. During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ contain trainable parameters. Note both $W_0$ and $\Delta W = BA$ are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For $h = W_0x$, our modified forward pass yields:
>
>$$h = W_0x + \Delta Wx = W_0x + BAx \$$

То есть, замораживаем исходную модель, вставляем низкоранговые адаптеры под желаемые матрицы весов в модели, обучаем эти адаптеры иммитировать обновления, которые в обычной ситуации приходили бы от градиентов. Думаю теперь, имея эти раасуждения и вышеприведенные формулы, вы можете наконец понять, что было изображено на той иллюстрации сверху. 

Откуда берется выйгрыш в оптимизации памяти и вычислений? Мы замораживаем исходную модель, и поэтому не храним градиенты и моменты оптимизатора для нее, и не проводим лишних вычисений. По сути, с небольшими оговорками, от базовой модели нам теперь нужен только инференс, что, правда, в случае с гиганскими моделями, тоже требует много железа. А обучаемые параметры адаптеров обычно занимают <1% от количества параметров исходной модели.


