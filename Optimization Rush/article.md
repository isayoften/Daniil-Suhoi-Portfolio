# LLM Оптимизация обучения. Обзор техник

![image](https://github.com/user-attachments/assets/4e3886a2-f8c7-4a74-919a-96276f9ef4b3)


Обучение больших языковых моделей (LLM) - ресурсоемкий процесс, требующий значительных вычислительных мощностей и времени. Эффективная оптимизация процесса обучения позволяет существенно снизить затраты, ускорить разработку и улучшить качество конечной модели. Данный гайд представляет собой комплексный обзор методов оптимизации, от выбора модели до тонкой настройки процесса обучения.

## Методы оптимизации
### 1. Оптимизация архитектуры и данных
Рассмотрим несколько довольно очевидных вещей.
#### 1.1. Выбор модели
Одну из самых простых оптимизаций, которую вы можете сделать в самом начале, это задаться вопросом: "А нужна ли для моей задачи убер-нейронка на 400B параметров, или достаточные метрики можно будет получить с какой-нибудь 8B Llama?" 
#### 1.2. Качество датасета
Качество датасета зачастую важнее его количества. Хорошо подготовленный датасет, даже в 10 раз меньший, чем случайно собранный, часто дает лучшие результаты. Это правило, конечно, не всеисчерпывающее и, например, в случае с претрейном LLM не будет работать в чистом виде.

С очевидными вещами закончили, переходим к более интересным техникам.
#### 1.3. Sequence packing
Все мы знаем, что последовательности бывают **разной длины** и из-за этого мы не можем составлять прямоугольные тензоры. 

Самое простое и распространенное решение - делать **паддинг**, динамический (то есть дополнять последовательности в отдельном батче), или вообще сразу дополнить все последовательности до максимальной длины контекста. Даже в случае динамического паддинга, как показывают [исследования](https://arxiv.org/abs/2107.02027), процент паддинга может составлять от 50% до целых 89%! То есть **больше половины вычислений проходят впустую.** 

Поэтому современные LLM тренируются более эффективным способом. **Основная идея** - запихивать данные во весь доступный context lenght. Если документ (или что-то другое) не влезают целиком в длину контекста, то обрезаем его и делаем следующий семпл с перекрытием ( то есть повторяем небольшой конец прошлого обрезанного документа). А если, наоборот, документ меньше длины контекста, то в оставшееся место пихаем следующий. 

Но, смышленый читатель возразит: *"Wait, wait, wait, а разве в таком случае у нас не будут во внимании трансформера сразу несколько документов, которые могут быть совсем не связаны друг с другом?"*. Вот тут и происходят дальнейшие разнообразные танцы с бубном. Одни пихают между документами специальный **токен-разделитель** в надежде, что модель сама поймет, что в контексте могут быть не связанные друг с другом документы. Более правильным решением будет сделать кастомную маску внимания под каждый семпл, содержащий не связанные документы:
![image](https://github.com/user-attachments/assets/44dc3e8b-0fb3-4a54-b4b8-cbe27f53f035)

Но это запарно и надо еще решить вопрос с совместимостю с FlashAttention (метод FA обсудим позже), так как по дефолту FA не поддерживает кастомные маски внимания. Еще по-хорошему нужно разобраться с позиционным кодированием - нам хотелось бы, чтобы для нового документа отсчет позиции возобновлялся. Я не уверен, нужен ли этот шаг для относительных позиционных энкодингов типа RoPE или ALiBi. 
![image](https://github.com/user-attachments/assets/f8a6869e-eda2-412e-88fa-e89853b94d97)

Отсылаю вас к интересному [посту](https://lweitkamp.github.io/posts/packing/#fn2) на эту тему для дальнейшего углубления.

### 2. Оптимизация памяти и вычислений
#### 2.1. PEFT (Parameter-Efficient Fine-Tuning)
PEFT возволяет сократить количество обучаемых параметров в десятки и даже сотни раз, остаавляя при этом качество, сопоставимое с full fine-tuning. Но давайте подумаем, почему это вообще возможно? 

PEFT представляет собой семейство методов, направленных на эффективную адаптацию крупномасштабных моделей путем обучения лишь небольшого подмножества параметров. Эти методы позволяют значительно сократить вычислительные затраты и объем требуемой памяти при сохранении качества, сопоставимой с полной тонкой настройкой.

LoRa является одним из наиболее эффективных методов PEFT.
- Идея: заморозим предобученную модель и будем обучать только адаптеры, которые представляют из себя низкоранговую аппроксимацию матриц весов
- Преимущество: количество обучаемых параметров зачастую сокращается до менее 1% от базовой модели.
- Ресурсы: [Hugging Face PEFT](https://huggingface.co/docs/peft/index).
