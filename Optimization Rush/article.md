# LLM Оптимизация обучения. Обзор техник

![image](https://github.com/user-attachments/assets/4e3886a2-f8c7-4a74-919a-96276f9ef4b3)


Обучение больших языковых моделей (LLM) - ресурсоемкий процесс, требующий значительных вычислительных мощностей и времени. Эффективная оптимизация процесса обучения позволяет существенно снизить затраты, ускорить разработку и улучшить качество конечной модели. Данный гайд представляет собой комплексный обзор методов оптимизации, от выбора модели до тонкой настройки процесса обучения.

## Методы оптимизации
### 1. Оптимизация архитектуры и данных
Рассмотрим несколько довольно очевидных вещей.
#### 1.1. Выбор модели
Одну из самых простых оптимизаций, которую вы можете сделать в самом начале, это задаться вопросом: "А нужна ли для моей задачи убер-нейронка на 400B параметров, или достаточные метрики можно будет получить с какой-нибудь 8B Llama?" 
#### 1.2. Качество датасета
Качество датасета зачастую важнее его количества. Хорошо подготовленный датасет, даже в 10 раз меньший, чем случайно собранный, часто дает лучшие результаты. Это правило, конечно, не всеисчерпывающее и, например, в случае с претрейном LLM не будет работать в чистом виде.

С очевидными вещами закончили, переходим к более интересным техникам.
#### 1.3. Sequence packing
Все мы знаем, что последовательности бывают **разной длины** и из-за этого мы не можем составлять прямоугольные тензоры. 

Самое простое и распространенное решение - делать **паддинг**, динамический (то есть дополнять последовательности в отдельном батче), или вообще сразу дополнить все последовательности до максимальной длины контекста. Даже в случае динамического паддинга, как показывают [исследования](https://arxiv.org/abs/2107.02027), процент паддинга может составлять от 50% до целых 89%! То есть **больше половины вычислений проходят впустую.** 

Поэтому современные LLM тренируются более эффективным способом. **Основная идея** - запихивать данные во весь доступный context lenght. Если документ (или что-то другое) не влезают целиком в длину контекста, то обрезаем его и делаем следующий семпл с перекрытием ( то есть повторяем небольшой конец прошлого обрезанного документа). А если, наоборот, документ меньше длины контекста, то в оставшееся место пихаем следующий. 

Но, смышленый читатель возразит: *"Wait, wait, wait, а разве в таком случае у нас не будут во внимании трансформера сразу несколько документов, которые могут быть совсем не связаны друг с другом?"*. Вот тут и происходят дальнейшие разнообразные танцы с бубном. Одни пихают между документами специальный **токен-разделитель** в надежде, что модель сама поймет, что в контексте могут быть не связанные друг с другом документы. Более правильным решением будет сделать кастомную маску внимания под каждый семпл, содержащий не связанные документы:

![image](https://github.com/user-attachments/assets/44dc3e8b-0fb3-4a54-b4b8-cbe27f53f035)

Но это запарно и надо еще решить вопрос с совместимостю с FlashAttention (метод FA обсудим позже), так как по дефолту FA не поддерживает кастомные маски внимания. Еще по-хорошему нужно разобраться с позиционным кодированием - нам хотелось бы, чтобы для нового документа отсчет позиции возобновлялся. Я не уверен, нужен ли этот шаг для относительных позиционных энкодингов типа RoPE или ALiBi. 

![image](https://github.com/user-attachments/assets/f8a6869e-eda2-412e-88fa-e89853b94d97)

Отсылаю вас к интересному [посту](https://lweitkamp.github.io/posts/packing/#fn2) на эту тему для дальнейшего углубления.

### 2. Оптимизация памяти и вычислений
#### 2.1. PEFT (Parameter-Efficient Fine-Tuning)
PEFT возволяет сократить количество обучаемых параметров в десятки и даже сотни раз, остаавляя при этом качество, сопоставимое с full fine-tuning. Но давайте подумаем, почему это вообще возможно? 

Сначала вспомним, как сделать самый элементарный PEFT - берем предобученную модель, которая повидала многое за время своего обучения, замораживаем бОльшую часть слоев, оставляя несколько последних (потому что первые слои отвечают за наиболее общие признаки, тогда как последние представляют специфичные для задачи фичи) и надеемся, что эти последние слои переучатся использовать богатую информацию из предыдущих для решения нашей специфичной задачи. Но, кажется, такое вертикальное деление модели по слоям не является очень эффективным - при малом количестве обучаемых слоев мы никак не адаптируем огромный пласт замороженных слоев, а при их разморозке опять получаем тяжеловесную махину. 

Существует много способов более эффективного дообучения, но я сегодня расскажу лишь об одном, завоевавшим наибольшую популярность - Low-Rank Adaptation (LoRA). Вообще у многих, когда речь заходит об эффективном файн тюнинге, LoRa подразумевается по умолчанию. 
