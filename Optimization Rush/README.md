# Optimization Rush
Моя статья на HF: https://huggingface.co/blog/Isayoften/optimization-rush

смотрите описание старых проектов в папке old_projects

текущий проект в работе. Заготовки кода можете посмотреть в last_project. Графики и результаты замеров скоро.

Цель - побенчить прирост скорости разных сетапов и методов оптимизации. Вот примерный план:
1. Single-gpu:
    - Берем 1xA100_80GB
    - Берем llama 3.2 1B
    - Начинаем накидывать фичи, замеряя прирост скорости (в токенах в секунду):
        1. Сначала без оптимизаций
        2. Добавляем mixed precision
        3. Добавляем кастом кернелы: FA, fused_adam, torch.compile, (liger_krenel?)
        4. Настраиваем оптимальний батч сайз (больше батч -> больше compute intensity, также более эфективно брать степени двойки)
        5. Хотим батч еще больше, но не влезает? Смотрим на activation checkpointing
        6. Упомянуть про возможные боттлнеки в загрузке данных и рассказать про num_workers и pin_memory в dataloader. (в нашем случае боттлнеков нет, можно не замерять это)
        7. Прочие возможные оптимизации, выходящие за рамки этого проекта: efficient seq packing(мы и так рандомные данные генерим максимального одинакового размера), quantization (например 8bit_adam), lora(только для файнтюна), gradient accumulation (не могу назвать это оптимизацией, скорее равносильный трейдофф времени и памяти)

2. DDP:
    - Оставляем все предыдущие оптимизации
    - Скейлимся до 2,4,8 A100 (но без нвлинков), замеряем прирост

3. FSDP:
    - Если хотим модельки больше, нужно разбивать по гпухам саму модель. (с помощью fsdp или model parallelism)
    - Берем llama 3.1 8b
    - Так как тут мы берем модель бОльшего размера, не имеет смысла сравнивать с предыдущими скоростями
    - Тут просто можно побенчить разное количество ГПУ (2,4,8 A100), разный интра конекшна (PCIe vs NVlink), разные типы шардирования (zero stage2 vs zero stage3), возможно, если достану кластер, можно multi-node побенчить. 
    - Ну и, если дойдут руки, побенчить tp, pp, cp. 








