# Optimization Rush: Максимально эффективное обучение LLM
#№ Введение
Обучение больших языковых моделей (LLM) - ресурсоемкий процесс, требующий значительных вычислительных мощностей и времени. Эффективная оптимизация процесса обучения позволяет существенно снизить затраты, ускорить разработку и улучшить качество конечной модели. Данный гайд представляет собой комплексный обзор методов оптимизации, от выбора модели до тонкой настройки процесса обучения.
 
## Методы оптимизации
### 1. Оптимизация архитектуры и данных
#### 1.1. Выбор модели
Оптимизация всегда должна начинаться с выбора подходящей модели. Для многих задач модель с несколькими миллиардами параметров может быть столь же эффективна, как и гигантская модель на сотни миллиардов параметров.
- В моем случае для генерации афоризмов вполне может хватить модели на 8B параметров.
- Совет: Начните с наименьшей модели, способной решить вашу задачу, и масштабируйте только при необходимости.
#### 1.2. Качество датасета
Качество датасета намного важнее его количества. Хорошо подготовленный датасет, даже в 10 раз меньший, чем случайно собранный, часто дает лучшие результаты.
- Преимущество: Обучение на качественном, но меньшем датасете может быть в 10 раз быстрее.
- Совет: Инвестируйте время в тщательную подготовку и очистку данных.
#### 1.3. Sequence packing
Эффективная упаковка последовательностей разной длины может значительно повысить производительность обучения.
- Проблема: Простой паддинг может привести к тому, что около половины вычислений будут производиться впустую.
- Простой вариант решения: Используйте сортировку последовательностей по длине и группировку в батчи для минимизации паддинга.
- Ресурс: Подробнее о методах упаковки можно прочитать [здесь](https://lweitkamp.github.io/posts/packing/#fn2)
### 2. Оптимизация памяти и вычислений
#### 2.1. PEFT (Parameter-Efficient Fine-Tuning)
......
#### 2.2. Quantization и QLoRa
Квантизация позволяет значительно уменьшить требования к памяти без существенной потери качества.
- Пример: Модель на 8B параметров в fp32 требует для обучения более 150 ГБ VRAM (32гб веса + 32гб градиенты + 64гб моменты Адама + активации), тогда как с QLoRa можно уложиться в ~10 ГБ.
- Идея: Квантизуем и заморозим предобученную LLM, будем обучать только адаптеры поверх модели.
- Статья: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
#### 2.3. Mixed Precision
Использование смешанной точности позволяет ускорить обучение и потенциально снизить требования к памяти.
- Идея: во время обучения некоторые операции можно проводить в 16bit без потери точности. 
- Ресурс: [Подробный гайд по mixed precision](https://residentmario.github.io/pytorch-training-performance-guide/mixed-precision.html)
#### 2.4. 8-bit optimizers
Квантизация оптимизаторов до 8 бит значительно снижает требования к памяти.
- Идея: квантизуем моменты оптимизатора. 
- Подробности: [8-битные оптимизаторы от Hugging Face](https://huggingface.co/docs/bitsandbytes/explanations/optimizers)
#### 2.5. torch.compile
Компиляция и оптимизация вычислительного графа для повышения производительности.
- Примечание: Несовместимо с QLoRa на момент написания гайда.
- Документация: [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)
#### 2.6. Efficient DataLoader
Оптимизация загрузки данных в DataLoader для предотвращения простоев GPU.
- Советы:
    - Используйте **num_workers** для параллельной подготовки батчей.
    - Применяйте **pin_memory** для ускорения копирования данных с CPU на GPU.
#### 2.7. Gradient Checkpointing
Техника, которая позволяет существенно снизить потребление памяти во время обучения за счет незначительного увеличения времени вычислений.
- Идея
    - Для рассчета градиентов на обратном проходе нам нужны активации с форвард пасса. Они все хранятся в памяти
    - Будем теперь сохранять не все активации, а только некоторые "чекпоинты"
    - При обратном проходе будем пересчитывать недостающие активации на основе сохраненных чекпоинтов.
- Trade-off: Не храним все активации одновременно в памяти, но тратим больше вычислений на перерасчет. 
- Гайд: Gradient Checkpointing в PyTorch

1. Оптимизация всегда должна начинаться в первую очередь с **выбора модели**. Если ваша задача не является комплексной, то модель с несколькими миллиардами параметров сможет справиться с ней почти так же хорошо, как какой-нибуль гигант на несколько сотен миллиардов параметров.
    - Для генерации афоризмов вполне должно хватить версии на 8B. 
2. **Качество датасета намного важнее его количества.** Хорошо подготовленный датасет, который в 10 раз меньше около рандомного скрапинга, даст качество зачастую даже лучше. И, главное, само обучение будет в 10 раз быстрее. Только придется потратить больше времени на обработку датасета.
3. **Sequence packing.** Все мы знаем, что последовательности бывают разной длины, поэтому приходится делать паддинг. Это самое простое, но самое неэффективное решение по упаковке и отправке данных в модель. Из-за паддинга около половины (зависит от разброса длин в датасете) вычислений проходят в пустую. Существует много способов и связанных с ними трудностей по эффективной упаковке батчей. Отсылаю вас к этому небольшому, но вполне исчерпывающему [посту](https://lweitkamp.github.io/posts/packing/#fn2)
    - Я выбрал компромисный путь - сортировку. То есть, отсортировал датасет по длине последовательности, нарезал этот датасет на батчи, потом перемешал порядок батчей. Благодаря этому минимизируется количество паддингов в батче.
4. **PEFT (Parameter-Efficient Fine-Tuning).** Думаю, не имеет смысла рассказывать про популярность и эффективность LoRa...
5. **Quantization.**: обучать напрямую квантизованную до 8 или 4 бит модель мы не можем. Но мы можем использовать LoRa поверх замороженной квантизованной модели. Гениально! На этой идее основана статья [QLoRa](https://arxiv.org/abs/2305.14314). Таким образом, модель на 8B в fp32 (опустим для простоты варианты с 16bit обучением) занимала бы ~32гб памяти, а еще нужно столько же хранить под градиенты(+32гб), плюс первый и второй моменты Адама(+64гб), плюс активации (+32, эта цифра может сильно варьироваться). А еще нужно батчи данных гонять. Итого 150гб+ VRAM!! Используя QLoRa, мы храним замороженную модель (то есть не нужны градиенты, моменты и тд для нее) в 4bit, что дает нам 8ГБ и еще сверху незначительный объем памяти под обучаение адаптеров. И, что самое главное, как заверяют авторы, получаем качество, сопоставимое с full fine-tuning.
6. **Mixed Precision**: Некоторые операции при обучении могут выполняться в 16bit без потери точности. Таким образом, мы экономим время (потому что операции в 16bit выполняются быстрее). С памятью сложнее, от mixed precision она может как увеличиться, так и уменьшиться. Очень советую этот [гайд](https://residentmario.github.io/pytorch-training-performance-guide/mixed-precision.html) по mixed precision. 
7. **8-bit optimizers**: помните, как печально хранить первый и второй моменты Адама? Давайте их тоже квантизуем! Как говорят авторы, потери в качестве около нулевые. За подробностями [сюда](https://huggingface.co/docs/bitsandbytes/explanations/optimizers)
8. **torch.compile**: Переводит вычислительный граф на производительный движок, компилирует и оптимизирует его. [Инфа1](https://pytorch.org/docs/stable/generated/torch.compile.html), [Инфа2](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html). Но, к сожалению, [несовместим](https://huggingface.co/docs/peft/developer_guides/torch_compile) со стратегией QLoRa
9. **Efficient DataLoader**: очевидно, но всё же. Используйте num_workers в DataLoader, таким образом сразу несколько процессов подготавливают батчи, чтобы GPU не ждал этого. Также используйте pin_memory, это ускорит копирование данных с CPU на GPU.
10. **Gradient Checkpointing**: очень крутая штука. Дело в том что, чтобы сделать бэквард пасс, нам нужны активации с форвард пасса, и обычно все эти активации хранятся в памяти и потом используются для вычисления градиентов на обратом пути. Мы можем сделать memory/speed trade-off, сохраним не все активации, а только несколько "чекпоинтов", а во время бэкварда просто пересчитаем все промежуточные активации между этими чекпоинтами. Тем самым мы не храним все активации одновременно в памяти, но тратим больше времени из-за дополнительных перевычислений. [Полезный гайд](https://residentmario.github.io/pytorch-training-performance-guide/gradient-checkpoints.html)
11. **Gradient Accumulation**: позволяет увеличить эффективный размер батча. Идея - будем обновлять веса не после каждого батча, а после каждых n батчей, накапливая градиенты на этих батчах. [Гайд](https://kozodoi.me/blog/20210219/gradient-accumulation)
