# Optimization Rush
Статья на HF: https://huggingface.co/blog/Isayoften/optimization-rush

## Цель
Погрузиться в теорию и практику эффективного обучения LLM, методов оптимизации.

## 1. FSDP+QLoRA
Во время изучения теории, мне пришла в голову идея объединить FSDP и QLoRA. На удивление, идея было достаточно новаторская и в коммьюнити разговоры о ней пошли всего за месяц-два до того, как я об этом задумался. Поэтому, помимо задачи попробовать всевозможные методы оптимизации на практике я решил сделать этот мини-проект.

Его целью не является обучить соту с минимальным компьютом, для меня было важно просто запустить обучение со столь объемным количеством методов и оверфитнуться (по заветам Карпатого) под один батч, чтобы убедиться, что обучение с высокой долей вероятности пошло. Сделать это мне удалось, но из-за новизны подхода совместимость FDSP и QLoRA в инструментах HF была еще не достаточно отлажена, поэтому я столкнулся с большим количеством багов и ошибок, которые, возможно, могли бы сильно повлиять на сходимость модели. 

По итогу я замечательно освоил теорию и методы оптимизации, а также потренировался пользоваться инструментами HF для этой задачи, что очень сильно помогло мне в следующем проекте (да и в целом это критически важные навыки в эру LLM)

## 2. SFT 
Цель - погрузиться в post-training, в SFT и DPO методы на практике. Здесь я хотел сделать full fine tuning базовой LLama-3.2-3B (без LoRA) с использованием FSDP, чтобы сделать из нее чатовую версию. 

Интересные методы оптимизации, не использованные ранее:
............... 

Результаты:
.............







