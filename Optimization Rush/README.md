# Optimization Rush: Максимально эффективное обучение LLM
## Введение
Обучение больших языковых моделей (LLM) - ресурсоемкий процесс, требующий значительных вычислительных мощностей и времени. Эффективная оптимизация процесса обучения позволяет существенно снизить затраты, ускорить разработку и улучшить качество конечной модели. Данный гайд представляет собой комплексный обзор методов оптимизации, от выбора модели до тонкой настройки процесса обучения.
 
## Методы оптимизации
### 1. Оптимизация архитектуры и данных
#### 1.1. Выбор модели
Оптимизация всегда должна начинаться с выбора подходящей модели. Для многих задач модель с несколькими миллиардами параметров может быть столь же эффективна, как и гигантская модель на сотни миллиардов параметров.
- В моем случае для генерации афоризмов вполне может хватить модели на 8B параметров.
- Совет: Начните с наименьшей модели, способной решить вашу задачу, и масштабируйте только при необходимости.
#### 1.2. Качество датасета
Качество датасета намного важнее его количества. Хорошо подготовленный датасет, даже в 10 раз меньший, чем случайно собранный, часто дает лучшие результаты.
- Преимущество: Обучение на качественном, но меньшем датасете может быть в 10 раз быстрее.
- Совет: Инвестируйте время в тщательную подготовку и очистку данных.
#### 1.3. Sequence packing
Эффективная упаковка последовательностей разной длины может значительно повысить производительность обучения.
- Проблема: Простой паддинг может привести к тому, что около половины вычислений будут производиться впустую.
- Простой вариант решения: Используйте сортировку последовательностей по длине и группировку в батчи для минимизации паддинга.
- Ресурс: Подробнее о методах упаковки можно прочитать [здесь](https://lweitkamp.github.io/posts/packing/#fn2)
### 2. Оптимизация памяти и вычислений
#### 2.1. PEFT (Parameter-Efficient Fine-Tuning)
..............................
#### 2.2. Quantization и QLoRa
Квантизация позволяет значительно уменьшить требования к памяти без существенной потери качества.
- Пример: Модель на 8B параметров в fp32 требует для обучения более 150 ГБ VRAM (32гб веса + 32гб градиенты + 64гб моменты Адама + активации), тогда как с QLoRa можно уложиться в ~10 ГБ.
- Идея: Квантизуем и заморозим предобученную LLM, будем обучать только адаптеры поверх модели.
- Статья: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
#### 2.3. Mixed Precision
Использование смешанной точности позволяет ускорить обучение и потенциально снизить требования к памяти.
- Идея: во время обучения некоторые операции можно проводить в 16bit без потери точности. 
- Ресурс: [Подробный гайд по mixed precision](https://residentmario.github.io/pytorch-training-performance-guide/mixed-precision.html)
#### 2.4. 8-bit optimizers
Квантизация оптимизаторов до 8 бит значительно снижает требования к памяти.
- Идея: квантизуем моменты оптимизатора. 
- Подробности: [8-битные оптимизаторы от Hugging Face](https://huggingface.co/docs/bitsandbytes/explanations/optimizers)
#### 2.5. torch.compile
Компиляция и оптимизация вычислительного графа для повышения производительности.
- Примечание: Несовместимо с QLoRa на момент написания гайда.
- Документация: [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)
#### 2.6. Efficient DataLoader
Оптимизация загрузки данных в DataLoader для предотвращения простоев GPU.
- Советы:
    - Используйте **num_workers** для параллельной подготовки батчей.
    - Применяйте **pin_memory** для ускорения копирования данных с CPU на GPU.
#### 2.7. Gradient Checkpointing
Техника, которая позволяет существенно снизить потребление памяти во время обучения за счет незначительного увеличения времени вычислений.
- Идея
    - Для рассчета градиентов на обратном проходе нам нужны активации с форвард пасса. Они все хранятся в памяти
    - Будем теперь сохранять не все активации, а только некоторые "чекпоинты"
    - При обратном проходе будем пересчитывать недостающие активации на основе сохраненных чекпоинтов.
- Trade-off: Не храним все активации одновременно в памяти, но тратим больше вычислений на перерасчет. 
- Гайд: [Gradient Checkpointing в PyTorch](https://residentmario.github.io/pytorch-training-performance-guide/gradient-checkpoints.html)
#### 2.8. Gradient Accumulation
Позволяет увеличить эффективный размер батча.
- Идея: Будем обновлять веса не после каждого батча, а после каждых n батчей, накапливая градиенты на этих батчах.
- Подробнее: [Gradient Accumulation в PyTorch](https://kozodoi.me/blog/20210219/gradient-accumulation)
### 3. Распределенные вычисления
