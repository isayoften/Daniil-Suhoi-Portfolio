# Optimization Rush
 **Цель** - максимально эффективно обучить LLM, используя все доступные методы оптимизации
## Методы
1. Оптимизация всегда должна начинаться в первую очередь с **выбора модели**. Если ваша задача не является комплексной, то модель с несколькими миллиардами параметров сможет справиться с ней почти так же хорошо, как какой-нибуль гигант на несколько сотен миллиардов параметров.
    - Для генерации афоризмов вполне должно хватить версии на 8B. 
2. **Качество датасета намного важнее его количества.** Хорошо подготовленный датасет, который в 10 раз меньше около рандомного скрапинга, даст качество зачастую даже лучше. И, главное, само обучение будет в 10 раз быстрее. Только придется потратить больше времени на обработку датасета.
3. **Sequence packing.** Все мы знаем, что последовательности бывают разной длины, поэтому приходится делать паддинг. Это самое простое, но самое неэффективное решение по упаковке и отправке данных в модель. Из-за паддинга около половины (зависит от разброса длин в датасете) вычислений проходят в пустую. Существует много способов и связанных с ними трудностей по эффективной упаковке батчей. Отсылаю вас к этому небольшому, но вполне исчерпывающему [посту](https://lweitkamp.github.io/posts/packing/#fn2)
    - Так как я не хотел смешивать в одной последовательности несколько рецептов и обрезать конец, что очень часто приводило бы к бессмысленным неоконченным рецептам, я выбрал стратегию сортировки. То есть, отсортировал датасет по длине последовательности, нарезал этот датасет на батчи, потом перемешал порядок батчей. Благодаря этому минимизируется количество паддингов в батче.
4. **PEFT (Parameter-Efficient Fine-Tuning).** Думаю, не имеет смысла рассказывать про популярность и эффективность LoRa...
5. **Quantization.**: обучать напрямую квантизованную до 8 или 4 бит модель мы не можем. Но мы можем использовать LoRa поверх замороженной квантизованной модели. Гениально! На этой идее основана статья [QLoRa](https://arxiv.org/abs/2305.14314). Таким образом, модель на 8B в fp32 (опустим для простоты варианты с 16bit обучением) занимала бы ~32гб памяти, а еще нужно столько же хранить под градиенты(+32гб), плюс первый и второй моменты Адама(+64гб), плюс активации (+32, эта цифра может сильно варьироваться). А еще нужно батчи данных гонять. Итого 150гб+ VRAM!! Используя QLoRa, мы храним замороженную модель (то есть не нужны градиенты, моменты и тд для нее) в 4bit, что дает нам 8ГБ и еще сверху незначительный объем памяти под обучаение адаптеров. И, что самое главное, как заверяют авторы, получаем качество, сопоставимое с full fine-tuning.
6. **Mixed Precision**: Некоторые операции при обучении могут выполняться в 16bit без потери точности. Таким образом, мы экономим время (потому что операции в 16bit выполняются быстрее). С памятью сложнее, от mixed precision она может как увеличиться, так и уменьшиться. Очень советую этот [гайд](https://residentmario.github.io/pytorch-training-performance-guide/mixed-precision.html) по mixed precision. 
7. **8-bit optimizers**: помните, как печально хранить первый и второй моменты Адама? Давайте их тоже квантизуем! Как говорят авторы, потери в качестве около нулевые. За подробностями [сюда](https://huggingface.co/docs/bitsandbytes/explanations/optimizers)
8. **torch.compile**: Переводит вычислительный граф на производительный движок, компилирует и оптимизирует его. [Инфа1](https://pytorch.org/docs/stable/generated/torch.compile.html), [Инфа2](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html). Но, к сожалению, [несовместим](https://huggingface.co/docs/peft/developer_guides/torch_compile) со стратегией QLoRa
9. **Efficient attention**
10. **Efficient DataLoader**
11. **Gradient Checkpointing**
12. **Gradient Accumulation**
