# GPT2 From Scratch

## Цель проекта
1. Собрать с нуля из спичек и желудей архитектуру GPT2, чтобы лучше понять подкапотную LLM
2. Сделать pretrain собранной модели на небольшом (10B токенов) дампе датасета FineWeb_edu, используя различные методы оптимизации, чтобы ускорить этот длительный процесс.

# Результаты
Все цели выполнены
1. Если хорошо знать теорию трансформеров, то имплементация архитектуры не представляет большого труда. Самое сложное это сделать эффективное распараллеливание в блоке multi head attention и соблюсти все тонкости оригинальной инициализации.
2. Обучать LLM почти на сыром пайторче - дело на из простых, но очень круто прокачивает понимание технических деталей (чего не скажешь про использование Trainer из HF). Для распределенных вычислений я пользуюсь библиотекой accelerate (немного казуал получается) от HF. Для проверки результатов я сверял свои лоссы с лоссами Андрея Карпатого, всё сошлось.

Мои:

![image](https://github.com/user-attachments/assets/e547e9df-243e-4628-a315-2b81e3fff07d)

Карпатый:

![image](https://github.com/user-attachments/assets/66ef3b3b-e296-4bd2-80d7-adf5ee76f019)

**Важные моменты**:
- Не дождался свободных A100, поэтому обучался на 4x4090. Из-за этого уменьшил размер эффективного батч сайза, иначе дороговато выходило для меня. Насколько можно судить по графикам и значениям лосса, это никак не повлияло на результат.
- **TODO**: не сделал автоматическую подгонку даталоадера, чтобы в случае возобновления обучения с сохраненными состояниями батчи продолжались на том месте, на котором закончились - надо воткнуть "skip_first_batches" от accelerate или настроить use_stateful_dataloader=True в DataLoaderConfiguration. 


# Environment

- OS: Ubuntu 24.05 CUDA 12.4
- Miniconda: Conda 24.7.1 Python 3.12.4 released Aug 22, 2024. https://docs.anaconda.com/miniconda/#quick-command-line-install
- conda create -n myenv python=3.11
- PyTorch 2.4.1: conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia    https://pytorch.org/get-started/locally/
- numpy==2.1.2 transformers==4.45.1 datasets==3.0.1 accelerate==1.0.0
